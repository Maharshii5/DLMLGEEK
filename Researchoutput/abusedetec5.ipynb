{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_amzDJrvpViN"
      },
      "outputs": [],
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import re\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# import warnings\n",
        "# from sklearn.svm import SVC\n",
        "# from sklearn.pipeline import make_pipeline\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "# from sklearn.ensemble import RandomForestClassifier\n",
        "# import xgboost as xgb\n",
        "# warnings.simplefilter(\"ignore\")\n",
        "# # Loading the dataset\n",
        "# data = pd.read_csv(\"Language Detection.csv\")\n",
        "# # value count for each language\n",
        "# data[\"Language\"].value_counts()\n",
        "# # separating the independent and dependant features\n",
        "# X = data[\"Text\"]\n",
        "# y = data[\"Language\"]\n",
        "# # converting categorical variables to numerical\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# le = LabelEncoder()\n",
        "# y = le.fit_transform(y)\n",
        "# # creating a list for appending the preprocessed text\n",
        "# data_list = []\n",
        "# # iterating through all the text\n",
        "# for text in X:\n",
        "#     # removing the symbols and numbers\n",
        "#     text = re.sub(r'[!@#$(),n\"%^*?:;~`0-9]', ' ', text)\n",
        "#     text = re.sub(r'[[]]', ' ', text)\n",
        "#     # converting the text to lower case\n",
        "#     text = text.lower()\n",
        "#     # appending to data_list\n",
        "#     data_list.append(text)\n",
        "# # creating bag of words using countvectorizer\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# cv = CountVectorizer()\n",
        "# X = cv.fit_transform(data_list).toarray()\n",
        "# #train test splitting\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
        "# #model creation and prediction\n",
        "# # from sklearn.naive_bayes import MultinomialNB\n",
        "# # nb_model = MultinomialNB()\n",
        "# # nb_model.fit(x_train, y_train)\n",
        "\n",
        "# # Get probabilities from Naive Bayes\n",
        "# # y_pred_nb = nb_model.predict(x_test)\n",
        "\n",
        "# # Calculate accuracy for the Naive Bayes model\n",
        "# # accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
        "# # print(f'Naive Bayes Model Accuracy: {accuracy_nb}')\n",
        "# # x_train_nb = nb_model.predict_proba(x_train)\n",
        "# # x_test_nb = nb_model.predict_proba(x_test)\n",
        "\n",
        "# # # XGBoost model\n",
        "# # xgb_model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "# # xgb_model.fit(x_train_nb, y_train)\n",
        "\n",
        "# # # Prediction using XGBoost\n",
        "# # y_pred_hybrid_xgb = xgb_model.predict(x_test_nb)\n",
        "\n",
        "# # # Evaluate the hybrid model\n",
        "# # accuracy_hybrid_nb = accuracy_score(y_test, y_pred_hybrid_nb)\n",
        "# # print(f'Hybrid Naive Bayes + XGBoost Model Accuracy: {accuracy_hybrid_nb}')\n",
        "# # recall_hybrid_nb = recall_score(y_test, y_pred_hybrid_nb, average='weighted')\n",
        "# # print(f'Hybrid Naive Bayes + XGBoost Model Recall: {recall_hybrid_nb}')\n",
        "# # precision_hybrid_nb = precision_score(y_test, y_pred_hybrid_nb, average='weighted')\n",
        "# # print(f'Hybrid Naive Bayes + XGBoost Model Precision: {precision_hybrid_nb}')\n",
        "# from sklearn.metrics import f1_score\n",
        "# # f1_hybrid_nb = f1_score(y_test, y_pred_hybrid_nb, average='weighted')\n",
        "# # print(f'Hybrid Naive Bayes + XGBoost Model F1 Score: {f1_hybrid_nb}')\n",
        "# # nb_model = MultinomialNB()\n",
        "# # nb_model.fit(x_train, y_train)\n",
        "\n",
        "# # # Get probabilities from Naive Bayes\n",
        "# # x_train_nb = nb_model.predict_proba(x_train)\n",
        "# # x_test_nb = nb_model.predict_proba(x_test)\n",
        "\n",
        "# # # Random Forest model\n",
        "# # rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "# # rf_model.fit(x_train_nb, y_train)\n",
        "\n",
        "# # # Prediction using Random Forest\n",
        "# # y_pred_hybrid_rf = rf_model.predict(x_test_nb)\n",
        "\n",
        "# # # Evaluate the hybrid model\n",
        "# # accuracy_hybrid_rf = accuracy_score(y_test, y_pred_hybrid_rf)\n",
        "# # print(f'Hybrid Naive Bayes + Random Forest Model Accuracy: {accuracy_hybrid_rf}')\n",
        "# # nb_model = MultinomialNB()\n",
        "# # nb_model.fit(x_train, y_train)\n",
        "\n",
        "# # # Get probabilities from Naive Bayes\n",
        "# # x_train_nb = nb_model.predict_proba(x_train)\n",
        "# # x_test_nb = nb_model.predict_proba(x_test)\n",
        "\n",
        "# # # SVM model\n",
        "# svm_model = make_pipeline(StandardScaler(with_mean=False), SVC(kernel='linear', probability=True))\n",
        "# svm_model.fit(x_train, y_train)\n",
        "\n",
        "# #Prediction using SVM\n",
        "# # y_pred_hybrid = svm_model.predict(x_test_nb)\n",
        "\n",
        "# # # Evaluate the hybrid model\n",
        "# # accuracy_hybrid = accuracy_score(y_test, y_pred_hybrid)\n",
        "# # print(f'Hybrid Model Accuracy: {accuracy_hybrid}')\n",
        "# # model = MultinomialNB()\n",
        "# # model.fit(x_train, y_train)\n",
        "# # # prediction\n",
        "# # y_pred = model.predict(x_test)\n",
        "# # # model evaluation\n",
        "# # from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "# # ac = accuracy_score(y_test, y_pred)\n",
        "# # print(ac)\n",
        "# # cm = confusion_matrix(y_test, y_pred)\n",
        "# # # visualising the confusion matrix\n",
        "# # plt.figure(figsize=(15,10))\n",
        "# # sns.heatmap(cm, annot = True)\n",
        "# # plt.show()\n",
        "# # # function for predicting language\n",
        "# # def predict(text):\n",
        "# #     x = cv.transform([text]).toarray()\n",
        "# #     lang = model.predict(x)\n",
        "# #     lang = le.inverse_transform(lang)\n",
        "# #     print(\"The langauge is in\",lang[0])\n",
        "# # precision_nb = precision_score(y_test, y_pred_nb, average='weighted')\n",
        "# # print(precision_nb)\n",
        "# # recall_nb = recall_score(y_test, y_pred_nb, average='weighted')\n",
        "# # print(recall_nb)\n",
        "# # f1_nb = f1_score(y_test, y_pred_nb, average='weighted')\n",
        "# # print(f1_nb)\n",
        "# # svm_model = SVC(kernel='linear')  # You can change the kernel type if needed\n",
        "# # svm_model.fit(x_train, y_train)\n",
        "\n",
        "# # # Predict using the SVM model\n",
        "# y_pred_svm = svm_model.predict(x_test)\n",
        "\n",
        "# # Calculate accuracy for the SVM model\n",
        "# accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "# print(f'SVM Model Accuracy: {accuracy_svm}')\n",
        "\n",
        "# # Calculate precision for the SVM model\n",
        "# precision_svm = precision_score(y_test, y_pred_svm, average='weighted')\n",
        "# print(f'SVM Model Precision: {precision_svm}')\n",
        "\n",
        "# # Calculate recall for the SVM model\n",
        "# recall_svm = recall_score(y_test, y_pred_svm, average='weighted')\n",
        "# print(f'SVM Model Recall: {recall_svm}')\n",
        "\n",
        "# # Calculate F1 score for the SVM model\n",
        "# f1_svm = f1_score(y_test, y_pred_svm, average='weighted')\n",
        "# print(f'SVM Model F1 Score: {f1_svm}')\n",
        "# precision_hybrid = precision_score(y_test, y_pred_hybrid, average='weighted')\n",
        "# print(f'Hybrid SVM + Naive Bayes Model Precision: {precision_hybrid}')\n",
        "\n",
        "# # Calculate recall for the hybrid model\n",
        "# recall_hybrid = recall_score(y_test, y_pred_hybrid, average='weighted')\n",
        "# print(f'Hybrid SVM + Naive Bayes Model Recall: {recall_hybrid}')\n",
        "\n",
        "# # Calculate F1 score for the hybrid model\n",
        "# f1_hybrid = f1_score(y_test, y_pred_hybrid, average='weighted')\n",
        "# print(f'Hybrid SVM + Naive Bayes Model F1 Score: {f1_hybrid}')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"Language Detection.csv\")\n",
        "\n",
        "# Separating the independent and dependent features\n",
        "X = data[\"Text\"]\n",
        "y = data[\"Language\"]\n",
        "\n",
        "# Converting categorical variables to numerical\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Preprocessing the text\n",
        "data_list = []\n",
        "for text in X:\n",
        "    text = re.sub(r'[!@#$(),\"%^*?:;~0-9]', ' ', text)  # Remove symbols and numbers\n",
        "    text = re.sub(r'[[]]', ' ', text)\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    data_list.append(text)\n",
        "\n",
        "# Creating bag of words using CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(data_list).toarray()\n",
        "\n",
        "# Train-test splitting\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Define the base learners\n",
        "base_learners = [\n",
        "    ('nb', MultinomialNB()),  # Naive Bayes\n",
        "    ('svm', SVC(probability=True)),  # Support Vector Machine with probability\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest\n",
        "]\n",
        "\n",
        "# Define the meta learner (Logistic Regression)\n",
        "meta_learner = LogisticRegression()\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner)\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_clf.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_stacking = stacking_clf.predict(x_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Accuracy of Stacking Classifier: {accuracy_stacking:.5f}\")\n",
        "\n",
        "# Optional: Confusion matrix for further analysis\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_stacking)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDzB4vRpr8vI",
        "outputId": "439366a9-e1d0-43a9-af91-f47d35154205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-63bcc2c6eef4>:28: FutureWarning: Possible nested set at position 1\n",
            "  text = re.sub(r'[[]]', ' ', text)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hRhQY0OBS_Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t_wT6dMHS_xL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vc4lmaqWTAVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import xgboost as xgb  # Importing XGBoost\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv(\"Language Detection.csv\")\n",
        "\n",
        "# Separating the independent and dependent features\n",
        "X = data[\"Text\"]\n",
        "y = data[\"Language\"]\n",
        "\n",
        "# Converting categorical variables to numerical\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "# Preprocessing the text\n",
        "data_list = []\n",
        "for text in X:\n",
        "    text = re.sub(r'[!@#$(),\"%^*?:;~0-9]', ' ', text)  # Remove symbols and numbers\n",
        "    text = re.sub(r'[[]]', ' ', text)\n",
        "    text = text.lower()  # Convert text to lowercase\n",
        "    data_list.append(text)\n",
        "\n",
        "# Creating bag of words using CountVectorizer\n",
        "cv = CountVectorizer()\n",
        "X = cv.fit_transform(data_list).toarray()\n",
        "\n",
        "# Train-test splitting\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
        "\n",
        "# Define the base learners with XGBoost added\n",
        "base_learners = [\n",
        "    ('nb', MultinomialNB()),  # Naive Bayes\n",
        "    ('svm', SVC(probability=True)),  # Support Vector Machine with probability\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),  # Random Forest\n",
        "    ('xgb', xgb.XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42))  # XGBoost\n",
        "]\n",
        "\n",
        "# Define the meta learner (Logistic Regression)\n",
        "meta_learner = LogisticRegression()\n",
        "\n",
        "# Create the stacking classifier\n",
        "stacking_clf = StackingClassifier(estimators=base_learners, final_estimator=meta_learner)\n",
        "\n",
        "# Train the stacking classifier\n",
        "stacking_clf.fit(x_train, y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred_stacking = stacking_clf.predict(x_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Accuracy of Stacking Classifier with XGBoost: {accuracy_stacking:.5f}\")\n",
        "\n",
        "# Optional: Confusion matrix for further analysis\n",
        "conf_matrix = confusion_matrix(y_test, y_pred_stacking)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ],
      "metadata": {
        "id": "RZGyvVZ1PxR9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "e5ef79d0-6a24-463f-8aed-d18f1cbdf7e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Language Detection.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-4d318f17dcb4>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Language Detection.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Separating the independent and dependent features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Language Detection.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6c7keripTP_9",
        "outputId": "e9ab6267-37f4-4a16-ae8e-026b5bb2443c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.26.4)\n",
            "Collecting nvidia-nccl-cu12 (from xgboost)\n",
            "  Downloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from xgboost) (1.13.1)\n",
            "Downloading xgboost-2.1.2-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.9/153.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.23.4-py3-none-manylinux2014_x86_64.whl (199.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.0/199.0 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nccl-cu12, xgboost\n",
            "Successfully installed nvidia-nccl-cu12-2.23.4 xgboost-2.1.2\n"
          ]
        }
      ]
    }
  ]
}